{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alzheimer's Detection Using CNN and MRI Scans\n",
    "\n",
    "Objective:\n",
    "\n",
    "Develop a Convolutional Neural Network (CNN) to detect Alzheimer's disease from MRI\n",
    "brain scans by classifying images into different categories such as Mild Cognitive\n",
    "Impairment (MCI), Alzheimerâ€™s Disease (AD), and Normal Controls (NC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 170, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder=r'D:\\soft computing\\project\\Axial\\AD'\n",
    "j=os.listdir(folder)\n",
    "img=cv2.imread(os.path.join(folder,j[0]))\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "Y_train = []\n",
    "image_size = 64 #As we are using AlexNet, the standard size is 227 x 227\n",
    "labels = ['AD','CN','MCI']\n",
    "for i in labels:\n",
    "    folderPath = os.path.join(r'D:\\soft computing\\project\\Axial',i)\n",
    "    for j in os.listdir(folderPath):\n",
    "        img = cv2.imread(os.path.join(folderPath,j))\n",
    "        img = cv2.resize(img,(image_size,image_size))\n",
    "        # img=cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        X_train.append(img)\n",
    "        Y_train.append(i)\n",
    "        \n",
    "X_train = np.array(X_train)\n",
    "Y_train = np.array(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5154, 64, 64, 3)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,Y_train = shuffle(X_train,Y_train,random_state=101)\n",
    "x_train,x_test,y_train,y_test = train_test_split(X_train,Y_train,test_size=0.55,random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_data1,x_test_data2,y_test_data1,y_test_data2=train_test_split(x_test,y_test,test_size=0.7,random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(850, 64, 64, 3)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_data1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2319, 64, 64, 3)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_new = []\n",
    "for i in y_train:\n",
    "    y_train_new.append(labels.index(i))\n",
    "y_train=np.eye(len(labels))[y_train_new]\n",
    "\n",
    "y_test_new = []\n",
    "for i in y_test:\n",
    "    y_test_new.append(labels.index(i))\n",
    "y_test=np.eye(len(labels))[y_test_new]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2319, 64, 64, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2319, 3)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Feature Shape: (2319, 1024)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define a simple 2D Convolution class\n",
    "class SimpleConv2D:\n",
    "    def __init__(self, input_shape, filter_size, num_filters, stride=1, padding='same'):\n",
    "        self.input_shape = input_shape  # Shape of the input image (height, width, channels)\n",
    "        self.filter_size = filter_size  # Size of the convolutional filter (height, width)\n",
    "        self.num_filters = num_filters  # Number of filters (output depth)\n",
    "        self.stride = stride  # Stride of the convolution operation\n",
    "        self.padding = padding  # Padding method ('same' or 'valid')\n",
    "        \n",
    "        # Initialize filters with random values and biases with zeros\n",
    "        self.filters = np.random.randn(filter_size[0], filter_size[1], input_shape[2], num_filters)\n",
    "        self.bias = np.zeros((num_filters,))\n",
    "    \n",
    "    # Define the convolution operation\n",
    "    def conv2d(self, input_image):\n",
    "        input_height, input_width, input_channels = self.input_shape\n",
    "        \n",
    "        # Apply padding if 'same' padding is specified\n",
    "        if self.padding == 'same':\n",
    "            pad_height = (self.filter_size[0] - 1) // 2\n",
    "            pad_width = (self.filter_size[1] - 1) // 2\n",
    "            input_image = np.pad(input_image, ((pad_height, pad_height), (pad_width, pad_width), (0, 0)), mode='constant')\n",
    "            input_height, input_width = input_image.shape[0], input_image.shape[1]\n",
    "        \n",
    "        # Calculate the dimensions of the output\n",
    "        output_height = (input_height - self.filter_size[0]) // self.stride + 1\n",
    "        output_width = (input_width - self.filter_size[1]) // self.stride + 1\n",
    "        \n",
    "        # Initialize the output feature map\n",
    "        output = np.zeros((output_height, output_width, self.num_filters))\n",
    "        \n",
    "        # Perform the convolution operation\n",
    "        for i in range(output_height):\n",
    "            for j in range(output_width):\n",
    "                # Extract the patch of the input image over which the filter slides\n",
    "                image_patch = input_image[i*self.stride:i*self.stride+self.filter_size[0], \n",
    "                                          j*self.stride:j*self.stride+self.filter_size[1], :]\n",
    "                for k in range(self.num_filters):\n",
    "                    # Convolve the filter with the image patch and add the bias\n",
    "                    output[i, j, k] = np.sum(image_patch * self.filters[:, :, :, k]) + self.bias[k]\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Define a simple Max Pooling class\n",
    "class SimpleMaxPooling:\n",
    "    def __init__(self, pool_size, stride):\n",
    "        self.pool_size = pool_size  # Size of the pooling window\n",
    "        self.stride = stride  # Stride of the pooling operation\n",
    "    \n",
    "    # Perform max pooling on the input image\n",
    "    def max_pool(self, input_image):\n",
    "        input_height, input_width, input_channels = input_image.shape\n",
    "        \n",
    "        # Calculate the dimensions of the output\n",
    "        output_height = (input_height - self.pool_size) // self.stride + 1\n",
    "        output_width = (input_width - self.pool_size) // self.stride + 1\n",
    "        \n",
    "        # Initialize the output\n",
    "        output = np.zeros((output_height, output_width, input_channels))\n",
    "        \n",
    "        # Perform max pooling\n",
    "        for i in range(output_height):\n",
    "            for j in range(output_width):\n",
    "                for c in range(input_channels):\n",
    "                    # Extract the patch of the input image\n",
    "                    region = input_image[i*self.stride:i*self.stride+self.pool_size, \n",
    "                                         j*self.stride:j*self.stride+self.pool_size, c]\n",
    "                    # Compute the maximum value in the patch\n",
    "                    output[i, j, c] = np.max(region)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Define the ReLU activation function\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Flatten the 3D input into a 1D array\n",
    "def flatten(input_image):\n",
    "    return input_image.flatten()\n",
    "\n",
    "# Define the AlexNet architecture with Dropout\n",
    "def alexnet(input_image):\n",
    "    # Layer 1: Conv1 (96 filters, 11x11, stride=4, padding='same') + ReLU + MaxPooling\n",
    "    conv1 = SimpleConv2D(input_shape=(64, 64, 3), filter_size=(4, 4), num_filters=96, stride=4, padding='same')\n",
    "    out1 = conv1.conv2d(input_image)  # Convolution operation\n",
    "    out1 = relu(out1)  # Apply ReLU activation\n",
    "    \n",
    "    maxpool1 = SimpleMaxPooling(pool_size=2, stride=2)  # Define max pooling layer\n",
    "    out1 = maxpool1.max_pool(out1)  # Apply max pooling\n",
    "    \n",
    "    # Layer 2: Conv2 (256 filters, 5x5, stride=1, padding='same') + ReLU + MaxPooling\n",
    "    conv2 = SimpleConv2D(input_shape=out1.shape, filter_size=(3, 3), num_filters=256, stride=2 , padding='same')\n",
    "    out2 = conv2.conv2d(out1)  # Convolution operation\n",
    "    out2 = relu(out2)  # Apply ReLU activation\n",
    "    \n",
    "    maxpool2 = SimpleMaxPooling(pool_size=2, stride=1)  # Define max pooling layer\n",
    "    out2 = maxpool2.max_pool(out2)  # Apply max pooling\n",
    "    \n",
    "    # Layer 3: Conv3 (384 filters, 3x3, stride=1, padding='same') + ReLU\n",
    "    conv3 = SimpleConv2D(input_shape=out2.shape, filter_size=(3, 3), num_filters=384, stride=1, padding='same')\n",
    "    out3 = conv3.conv2d(out2)  # Convolution operation\n",
    "    out3 = relu(out3)  # Apply ReLU activation\n",
    "    \n",
    "    # Layer 4: Conv4 (384 filters, 3x3, stride=1, padding='same') + ReLU\n",
    "    conv4 = SimpleConv2D(input_shape=out3.shape, filter_size=(3, 3), num_filters=384, stride=1, padding='same')\n",
    "    out4 = conv4.conv2d(out3)  # Convolution operation\n",
    "    out4 = relu(out4)  # Apply ReLU activation\n",
    "    \n",
    "    # Layer 5: Conv5 (256 filters, 3x3, stride=1, padding='same') + ReLU + MaxPooling\n",
    "    conv5 = SimpleConv2D(input_shape=out4.shape, filter_size=(3, 3), num_filters=256, stride=1, padding='same')\n",
    "    out5 = conv5.conv2d(out4)  # Convolution operation\n",
    "    out5 = relu(out5)  # Apply ReLU activation\n",
    "    \n",
    "    maxpool3 = SimpleMaxPooling(pool_size=2, stride=1)  # Define max pooling layer\n",
    "    out5 = maxpool3.max_pool(out5)  # Apply max pooling\n",
    "    \n",
    "    # Flatten the output for fully connected layers\n",
    "    flattened = flatten(out5)\n",
    "    \n",
    "    # Apply dropout to the flattened output\n",
    "    # flattened = dropout(flattened, drop_probability=dropout_probability)\n",
    "    return flattened\n",
    "# Example input (randomly initialized 227x227x3 image)\n",
    "result = []\n",
    "for img in x_train:\n",
    "    # Process each input image through the AlexNet model\n",
    "    temp = alexnet(img)\n",
    "    result.append(temp)\n",
    "\n",
    "# Convert the results to a numpy array\n",
    "result = np.array(result)\n",
    "print(\"Final Feature Shape:\", result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final size of test data: (850, 1024)\n"
     ]
    }
   ],
   "source": [
    "x_test_flat=[]\n",
    "for img in x_test_data1:\n",
    "    temp=alexnet(img)\n",
    "    x_test_flat.append(temp)\n",
    "x_test_flat=np.array(x_test_flat)\n",
    "print(\"Final size of test data:\",x_test_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2319, 1024)\n"
     ]
    }
   ],
   "source": [
    "result.shape\n",
    "x_train_flat=result\n",
    "print(x_train_flat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bpn with Relu and softmax functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 19.3977\n",
      "Epoch 10/50, Loss: 1.0959\n",
      "Epoch 20/50, Loss: 1.0920\n",
      "Epoch 30/50, Loss: 1.0883\n",
      "Epoch 40/50, Loss: 1.0849\n",
      "Epoch 50/50, Loss: 1.0816\n",
      "Training Accuracy: 0.4955\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Activation functions\n",
    "def relu(x):\n",
    "    \"\"\"ReLU activation function: returns max(0, x) for each element in x.\"\"\"\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    \"\"\"Derivative of the ReLU activation function: returns 1 for x > 0, and 0 otherwise.\"\"\"\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Softmax activation function: normalizes the input into probabilities.\"\"\"\n",
    "    exps = np.exp(x - np.max(x, axis=1, keepdims=True))  # Subtract max(x) for numerical stability\n",
    "    return exps / np.sum(exps, axis=1, keepdims=True)  # Normalize to get probabilities\n",
    "\n",
    "# Cross-entropy loss function\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    \"\"\"Cross-entropy loss function for multi-class classification.\"\"\"\n",
    "    epsilon = 1e-12  # Small epsilon to avoid log(0) which results in NaN\n",
    "    y_pred = np.clip(y_pred, epsilon, 1. - epsilon)  # Clip values to avoid extreme log values\n",
    "    return -np.mean(np.sum(y_true * np.log(y_pred), axis=1))  # Compute average loss\n",
    "\n",
    "# Model initialization\n",
    "def initialize_weights(input_size, hidden_size, output_size):\n",
    "    \"\"\"Initialize weights and biases for the model.\"\"\"\n",
    "    W1 = np.random.randn(input_size, hidden_size) * 0.01  # Small random values for input to hidden weights\n",
    "    b1 = np.zeros((1, hidden_size))  # Biases for the hidden layer initialized to zeros\n",
    "    W2 = np.random.randn(hidden_size, output_size) * 0.01  # Small random values for hidden to output weights\n",
    "    b2 = np.zeros((1, output_size))  # Biases for the output layer initialized to zeros\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "# Forward propagation\n",
    "def forward_propagation(X, W1, b1, W2, b2):\n",
    "    \"\"\"Perform forward propagation to compute predictions.\"\"\"\n",
    "    Z1 = np.dot(X, W1) + b1  # Compute input to the hidden layer (Z1)\n",
    "    A1 = relu(Z1)  # Apply ReLU activation to hidden layer input (A1)\n",
    "    Z2 = np.dot(A1, W2) + b2  # Compute input to the output layer (Z2)\n",
    "    A2 = softmax(Z2)  # Apply Softmax to get output probabilities (A2)\n",
    "    return Z1, A1, Z2, A2  # Return intermediate values for use in backward propagation\n",
    "\n",
    "# Backward propagation\n",
    "def backward_propagation(X, Y, Z1, A1, A2, W2):\n",
    "    \"\"\"Compute gradients during backward propagation.\"\"\"\n",
    "    m = X.shape[0]  # Number of training examples (samples)\n",
    "\n",
    "    # Output layer gradients (dW2, db2)\n",
    "    dZ2 = A2 - Y  # Derivative of loss with respect to output (cross-entropy)\n",
    "    dW2 = np.dot(A1.T, dZ2) / m  # Gradient of the weights for the output layer\n",
    "    db2 = np.sum(dZ2, axis=0, keepdims=True) / m  # Gradient of the biases for the output layer\n",
    "\n",
    "    # Hidden layer gradients (dW1, db1)\n",
    "    dA1 = np.dot(dZ2, W2.T)  # Gradient with respect to hidden layer's activations\n",
    "    dZ1 = dA1 * relu_derivative(Z1)  # Derivative of ReLU for hidden layer\n",
    "    dW1 = np.dot(X.T, dZ1) / m  # Gradient of the weights for the hidden layer\n",
    "    db1 = np.sum(dZ1, axis=0, keepdims=True) / m  # Gradient of the biases for the hidden layer\n",
    "\n",
    "    return dW1, db1, dW2, db2  # Return all gradients for weight and bias updates\n",
    "\n",
    "# Training the model\n",
    "def train(X_train, Y_train, input_size, hidden_size, output_size, epochs, learning_rate):\n",
    "    \"\"\"Train the neural network model using forward and backward propagation.\"\"\"\n",
    "    # Initialize model weights and biases\n",
    "    W1, b1, W2, b2 = initialize_weights(input_size, hidden_size, output_size)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass to compute predictions\n",
    "        Z1, A1, Z2, A2 = forward_propagation(X_train, W1, b1, W2, b2)\n",
    "\n",
    "        # Compute the loss (cross-entropy)\n",
    "        loss = cross_entropy_loss(Y_train, A2)\n",
    "\n",
    "        # Backward pass to compute gradients\n",
    "        dW1, db1, dW2, db2 = backward_propagation(X_train, Y_train, Z1, A1, A2, W2)\n",
    "\n",
    "        # Update weights and biases using the gradients\n",
    "        W1 -= learning_rate * dW1  # Update weights for the hidden layer\n",
    "        b1 -= learning_rate * db1  # Update biases for the hidden layer\n",
    "        W2 -= learning_rate * dW2  # Update weights for the output layer\n",
    "        b2 -= learning_rate * db2  # Update biases for the output layer\n",
    "\n",
    "        # Print loss every 10 epochs or at the first epoch\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}\")\n",
    "\n",
    "    return W1, b1, W2, b2  # Return the trained weights and biases\n",
    "\n",
    "# Prediction function\n",
    "def predict(X, W1, b1, W2, b2):\n",
    "    \"\"\"Make predictions for the input data X using the trained model.\"\"\"\n",
    "    _, _, _, A2 = forward_propagation(X, W1, b1, W2, b2)  # Perform forward propagation\n",
    "    return np.argmax(A2, axis=1)  # Return the index of the maximum probability (the predicted class)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Data dimensions for a dataset with 1024 input features and 3 output classes\n",
    "    input_size = 1024  # Number of features (input layer size)\n",
    "    hidden_size = 512  # Number of neurons in the hidden layer\n",
    "    output_size = 3  # Number of classes in the output layer (Alzheimer classification with 3 classes)\n",
    "\n",
    "    # Synthetic data for testing (replace with actual data)\n",
    "    X_train = result  # Training data\n",
    "    Y_train = y_train  # One-hot encoded labels for training data\n",
    "\n",
    "    # Hyperparameters for training\n",
    "    epochs = 50  # Number of epochs to train the model\n",
    "    learning_rate = 0.01  # Learning rate for weight updates\n",
    "\n",
    "    # Train the model and obtain the final weights and biases\n",
    "    W1, b1, W2, b2 = train(X_train, Y_train, input_size, hidden_size, output_size, epochs, learning_rate)\n",
    "\n",
    "    # Make predictions on the training data (or test data)\n",
    "    predictions = predict(X_train, W1, b1, W2, b2)\n",
    "\n",
    "    # Calculate the training accuracy by comparing predicted classes with true classes\n",
    "    accuracy = np.mean(predictions == y_train_new)  # y_train_new contains the actual class labels\n",
    "    print(f\"Training Accuracy: {accuracy:.4f}\")  # Output the accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for ReLU + Softmax Network: 49.88%\n"
     ]
    }
   ],
   "source": [
    "# Function to predict class labels\n",
    "def predict_relu_softmax(X):\n",
    "    # Forward pass\n",
    "    z1 = np.dot(X, W1) + b1  # Hidden layer input\n",
    "    a1 = relu(z1)  # Hidden layer output (ReLU activation)\n",
    "    z2 = np.dot(a1, W2) + b2  # Output layer input\n",
    "    a2 = softmax(z2)  # Output layer output (Softmax activation)\n",
    "    return a2\n",
    "\n",
    "# Function to calculate accuracy\n",
    "def calculate_accuracy_relu_softmax(X_test, Y_test):\n",
    "    # Get predictions\n",
    "    predictions = predict_relu_softmax(X_test)\n",
    "    \n",
    "    # Convert predictions to class labels\n",
    "    predicted_classes = np.argmax(predictions, axis=1)  # Choose the class with the highest probability\n",
    "    true_classes = np.argmax(Y_test, axis=1)  # Convert one-hot encoded labels to class labels\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = np.mean(predicted_classes == true_classes)  # Percentage of correct predictions\n",
    "    return accuracy\n",
    "\n",
    "# Example usage:\n",
    "accuracy_relu_softmax = calculate_accuracy_relu_softmax(x_test_flat, y_test_data1)  # X_test_flat and y_test are your testing data and labels\n",
    "print(f\"Accuracy for ReLU + Softmax Network: {accuracy_relu_softmax * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pure Bpn (Uses sigmoid function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 0.6783\n",
      "Epoch 10/50, Loss: 1.0620\n",
      "Epoch 20/50, Loss: 1.0158\n",
      "Epoch 30/50, Loss: 0.9996\n",
      "Epoch 40/50, Loss: 0.9795\n",
      "Epoch 50/50, Loss: 1.0301\n",
      "BPN Training Accuracy: 0.4955\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Activation functions\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid activation function: computes 1 / (1 + exp(-x)) with numerical stability.\"\"\"\n",
    "    x = np.clip(x, -15, 15)  # Clip the input to avoid overflow issues in exp\n",
    "    return 1 / (1 + np.exp(-x))  # Sigmoid function: squashes input to range (0, 1)\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    \"\"\"Derivative of the sigmoid activation function.\"\"\"\n",
    "    return x * (1 - x)  # Sigmoid derivative, using the output of sigmoid function\n",
    "\n",
    "# Cross-entropy loss function\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    \"\"\"Computes the cross-entropy loss, which is used for multi-class classification.\"\"\"\n",
    "    epsilon = 1e-12  # Small epsilon to avoid log(0) which can result in NaN\n",
    "    y_pred = np.clip(y_pred, epsilon, 1. - epsilon)  # Clip predicted values to avoid log(0)\n",
    "    return -np.mean(np.sum(y_true * np.log(y_pred), axis=1))  # Cross-entropy loss calculation\n",
    "\n",
    "# Model initialization function\n",
    "def initialize_weights_bpn(input_size, hidden_size, output_size):\n",
    "    \"\"\"Initializes the weights and biases for the model.\"\"\"\n",
    "    W1 = np.random.randn(input_size, hidden_size) * 0.01  # Random weights for input to hidden layer\n",
    "    b1 = np.zeros((1, hidden_size))  # Initialize biases for the hidden layer to zeros\n",
    "    W2 = np.random.randn(hidden_size, output_size) * 0.01  # Random weights for hidden to output layer\n",
    "    b2 = np.zeros((1, output_size))  # Initialize biases for the output layer to zeros\n",
    "    return W1, b1, W2, b2  # Return the initialized weights and biases\n",
    "\n",
    "# Forward propagation function\n",
    "def forward_propagation_bpn(X, W1, b1, W2, b2):\n",
    "    \"\"\"Performs forward propagation through the network.\"\"\"\n",
    "    Z1 = np.dot(X, W1) + b1  # Linear combination for input to hidden layer\n",
    "    A1 = sigmoid(Z1)  # Apply sigmoid activation to the hidden layer\n",
    "    Z2 = np.dot(A1, W2) + b2  # Linear combination for hidden to output layer\n",
    "    A2 = sigmoid(Z2)  # Apply sigmoid activation to the output layer\n",
    "    return Z1, A1, Z2, A2  # Return intermediate values for use in backward propagation\n",
    "\n",
    "# Backward propagation function\n",
    "def backward_propagation_bpn(X, Y, Z1, A1, A2, W2):\n",
    "    \"\"\"Performs backward propagation to compute gradients.\"\"\"\n",
    "    m = X.shape[0]  # Number of training samples\n",
    "\n",
    "    # Output layer gradients (dW2, db2)\n",
    "    dZ2 = A2 - Y  # Derivative of the loss w.r.t output (cross-entropy)\n",
    "    dW2 = np.dot(A1.T, dZ2) / m  # Gradient of weights from hidden to output layer\n",
    "    db2 = np.sum(dZ2, axis=0, keepdims=True) / m  # Gradient of biases for output layer\n",
    "\n",
    "    # Hidden layer gradients (dW1, db1)\n",
    "    dA1 = np.dot(dZ2, W2.T)  # Gradient w.r.t hidden layer's activations\n",
    "    dZ1 = dA1 * sigmoid_derivative(A1)  # Derivative of the sigmoid activation for hidden layer\n",
    "    dW1 = np.dot(X.T, dZ1) / m  # Gradient of weights from input to hidden layer\n",
    "    db1 = np.sum(dZ1, axis=0, keepdims=True) / m  # Gradient of biases for hidden layer\n",
    "\n",
    "    return dW1, db1, dW2, db2  # Return the computed gradients\n",
    "\n",
    "# Training function\n",
    "def train_bpn(X_train, Y_train, input_size, hidden_size, output_size, epochs, learning_rate):\n",
    "    \"\"\"Trains the neural network model using forward and backward propagation.\"\"\"\n",
    "    # Initialize weights and biases\n",
    "    W1, b1, W2, b2 = initialize_weights_bpn(input_size, hidden_size, output_size)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass: compute predictions\n",
    "        Z1, A1, Z2, A2 = forward_propagation_bpn(X_train, W1, b1, W2, b2)\n",
    "\n",
    "        # Compute the loss using cross-entropy\n",
    "        loss = cross_entropy_loss(Y_train, A2)\n",
    "\n",
    "        # Backward pass: compute gradients\n",
    "        dW1, db1, dW2, db2 = backward_propagation_bpn(X_train, Y_train, Z1, A1, A2, W2)\n",
    "\n",
    "        # Update weights and biases using gradient descent\n",
    "        W1 -= learning_rate * dW1  # Update weights for the input to hidden layer\n",
    "        b1 -= learning_rate * db1  # Update biases for the hidden layer\n",
    "        W2 -= learning_rate * dW2  # Update weights for the hidden to output layer\n",
    "        b2 -= learning_rate * db2  # Update biases for the output layer\n",
    "\n",
    "        # Print the loss every 10 epochs or at the first epoch\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}\")\n",
    "\n",
    "    return W1, b1, W2, b2  # Return the trained weights and biases\n",
    "\n",
    "# Prediction function\n",
    "def predict_bpn(X, W1, b1, W2, b2):\n",
    "    \"\"\"Makes predictions using the trained model.\"\"\"\n",
    "    _, _, _, A2 = forward_propagation_bpn(X, W1, b1, W2, b2)  # Perform forward propagation\n",
    "    return np.argmax(A2, axis=1)  # Return the index of the highest probability (the predicted class)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Define data dimensions\n",
    "    input_size = 1024  # Number of features (input layer size)\n",
    "    hidden_size = 512  # Number of neurons in the hidden layer\n",
    "    output_size = 3  # Number of classes (Alzheimer disease classification with 3 classes)\n",
    "\n",
    "    # Synthetic data for testing (replace with actual data)\n",
    "    X_train = result  # Training data\n",
    "    Y_train = y_train  # One-hot encoded labels for training data\n",
    "\n",
    "    # Hyperparameters for training\n",
    "    epochs = 50  # Number of epochs for training\n",
    "    learning_rate = 0.01  # Learning rate for weight updates\n",
    "\n",
    "    # Train the BPN model\n",
    "    W1_bpn, b1_bpn, W2_bpn, b2_bpn = train_bpn(X_train, Y_train, input_size, hidden_size, output_size, epochs, learning_rate)\n",
    "\n",
    "    # Make predictions on the training data (or test data)\n",
    "    predictions_bpn = predict_bpn(X_train, W1_bpn, b1_bpn, W2_bpn, b2_bpn)\n",
    "\n",
    "    # Calculate accuracy by comparing predicted classes with actual labels\n",
    "    accuracy_bpn = np.mean(predictions_bpn == y_train_new)  # y_train_new contains the true labels\n",
    "    print(f\"BPN Training Accuracy: {accuracy_bpn:.4f}\")  # Output the accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Pure BPN: 22.71%\n"
     ]
    }
   ],
   "source": [
    "# Function to predict class labels\n",
    "def predict_bpn(X):\n",
    "    # Forward pass\n",
    "    z1 = np.dot(X, W1) + b1  # Hidden layer input\n",
    "    a1 = sigmoid(z1)  # Hidden layer output (Sigmoid activation)\n",
    "    z2 = np.dot(a1, W2) + b2  # Output layer input\n",
    "    a2 = sigmoid(z2)  # Output layer output (Sigmoid activation)\n",
    "    return a2\n",
    "\n",
    "# Function to calculate accuracy\n",
    "def calculate_accuracy_bpn(X_test, Y_test):\n",
    "    # Get predictions\n",
    "    predictions = predict_bpn(X_test)\n",
    "    \n",
    "    # Convert predictions to class labels\n",
    "    predicted_classes = np.argAmax(predictions, axis=1)  # Choose the class with the highest probability\n",
    "    true_classes = np.argmax(Y_test, axis=1)  # Convert one-hot encoded labels to class labels\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = np.mean(predicted_classes == true_classes)  # Percentage of correct predictions\n",
    "    return accuracy\n",
    "\n",
    "# Example usage:\n",
    "accuracy_bpn = calculate_accuracy_bpn(x_test_flat, y_test_data1)  # X_test_flat and y_test are your testing data and labels\n",
    "print(f\"Accuracy for Pure BPN: {accuracy_bpn * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
